{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6cd9d5ad61ffe3b8858769f20a5f9493f024a56"
   },
   "source": [
    "## Model Parameters\n",
    "We might want to adjust these later (or do some hyperparameter optimizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "301a5d939c566d1487a049bb2554d09b592b18b1"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "EDGE_CROP = 16\n",
    "\n",
    "NB_EPOCHS = 5\n",
    "\n",
    "GAUSSIAN_NOISE = 0.1\n",
    "\n",
    "UPSAMPLE_MODE = 'SIMPLE'\n",
    "\n",
    "# downsampling inside the network\n",
    "NET_SCALING = None\n",
    "\n",
    "# downsampling in preprocessing\n",
    "IMG_SCALING = (1, 1)\n",
    "\n",
    "# number of validation images to use\n",
    "VALID_IMG_COUNT = 400\n",
    "\n",
    "# maximum number of steps_per_epoch in training\n",
    "MAX_TRAIN_STEPS = 200\n",
    "\n",
    "AUGMENT_BRIGHTNESS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc; gc.enable() # memory is tight\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util import montage2d as montage\n",
    "from skimage.morphology import label, binary_opening, disk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm_notebook #progress bar\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import models, layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "montage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\n",
    "\n",
    "def multi_rle_encode(img):\n",
    "    labels = label(img[:, :, 0])\n",
    "    return [rle_encode(labels==k) for k in np.unique(labels[labels>0])]\n",
    "\n",
    "# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "def masks_as_image(in_mask_list):\n",
    "    # Take the individual ship masks and create a single mask array for all ships\n",
    "    all_masks = np.zeros((768, 768), dtype = np.int16)\n",
    "    #if isinstance(in_mask_list, list):\n",
    "    for mask in in_mask_list:\n",
    "        if isinstance(mask, str):\n",
    "            all_masks += rle_decode(mask)\n",
    "    return np.expand_dims(all_masks, -1)\n",
    "\n",
    "def sample_ships(in_df, base_rep_val=1500):\n",
    "    if in_df['ships'].values[0]==0:\n",
    "        return in_df.sample(base_rep_val//3) # even more strongly undersample no ships\n",
    "    else:\n",
    "        return in_df.sample(base_rep_val, replace=(in_df.shape[0]<base_rep_val))\n",
    "\n",
    "def make_image_gen(in_df, batch_size = BATCH_SIZE):\n",
    "    all_batches = list(in_df.groupby('ImageId'))\n",
    "    out_rgb = []\n",
    "    out_mask = []\n",
    "    while True:\n",
    "        np.random.shuffle(all_batches)\n",
    "        for c_img_id, c_masks in all_batches:\n",
    "            rgb_path = os.path.join(train_image_dir, c_img_id)\n",
    "            c_img = imread(rgb_path)\n",
    "            c_mask = masks_as_image(c_masks['EncodedPixels'].values)\n",
    "            if IMG_SCALING is not None:\n",
    "                c_img = c_img[::IMG_SCALING[0], ::IMG_SCALING[1]]\n",
    "                c_mask = c_mask[::IMG_SCALING[0], ::IMG_SCALING[1]]\n",
    "            out_rgb += [c_img]\n",
    "            out_mask += [c_mask]\n",
    "            if len(out_rgb)>=batch_size:\n",
    "                yield np.stack(out_rgb, 0)/255.0, np.stack(out_mask, 0)\n",
    "                out_rgb, out_mask=[], []\n",
    "                \n",
    "def create_aug_gen(in_gen, seed = None):\n",
    "    np.random.seed(seed if seed is not None else np.random.choice(range(9999)))\n",
    "    for in_x, in_y in in_gen:\n",
    "        seed = np.random.choice(range(9999))\n",
    "        # keep the seeds syncronized otherwise the augmentation to the images is different from the masks\n",
    "        g_x = image_gen.flow(255*in_x, \n",
    "                             batch_size = in_x.shape[0], \n",
    "                             seed = seed, \n",
    "                             shuffle=True)\n",
    "        g_y = label_gen.flow(in_y, \n",
    "                             batch_size = in_x.shape[0], \n",
    "                             seed = seed, \n",
    "                             shuffle=True)\n",
    "\n",
    "        yield next(g_x)/255.0, next(g_y)\n",
    "        \n",
    "def upsample_conv(filters, kernel_size, strides, padding):\n",
    "    return layers.Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding)\n",
    "\n",
    "def upsample_simple(filters, kernel_size, strides, padding):\n",
    "    return layers.UpSampling2D(strides)        \n",
    "    \n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n",
    "\n",
    "def dice_p_bce(in_gt, in_pred):\n",
    "    return 1e-3*binary_crossentropy(in_gt, in_pred) - dice_coef(in_gt, in_pred)\n",
    "\n",
    "def true_positive_rate(y_true, y_pred):\n",
    "    return K.sum(K.flatten(y_true)*K.flatten(K.round(y_pred)))/K.sum(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_dir = '../HDD/kaggle_datasets/ship_detection'\n",
    "train_image_dir = os.path.join(ship_dir, 'train')\n",
    "test_image_dir = os.path.join(ship_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "3ca7119188fbb4c6540d9df55f5833b55435287e"
   },
   "outputs": [],
   "source": [
    "masks = pd.read_csv(os.path.join('../HDD/kaggle_datasets/ship_detection/',\n",
    "                                 'train_ship_segmentations.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "40cb72e241c0c3d8bc245b4e3c663b4a835b0011"
   },
   "source": [
    "# Split into training and validation groups\n",
    "We stratify by the number of boats appearing so we have nice balances in each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "c4f008bf6898518fd371de013418f936edaa09f8"
   },
   "outputs": [],
   "source": [
    "masks['ships'] = masks['EncodedPixels'].map(lambda c_row: 1 if isinstance(c_row, str) else 0)\n",
    "unique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\n",
    "unique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\n",
    "unique_img_ids['has_ship_vec'] = unique_img_ids['has_ship'].map(lambda x: [x])\n",
    "# some files are too small/corrupt\n",
    "unique_img_ids['file_size_kb'] = unique_img_ids['ImageId'].map(lambda c_img_id: \n",
    "                                                               os.stat(os.path.join(train_image_dir, \n",
    "                                                                                    c_img_id)).st_size/1024)\n",
    "unique_img_ids = unique_img_ids[unique_img_ids['file_size_kb']>50] # keep only 50kb files\n",
    "masks.drop(['ships'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "871720221ac25f7f9408bfe01aeb4ccb95edbd1f"
   },
   "outputs": [],
   "source": [
    "train_ids, valid_ids = train_test_split(unique_img_ids, \n",
    "                 test_size = 0.3, \n",
    "                 stratify = unique_img_ids['ships'])\n",
    "\n",
    "train_df = pd.merge(masks, train_ids)\n",
    "valid_df = pd.merge(masks, valid_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef8115a80749ac47f295e9a70217a5553970c2b3"
   },
   "source": [
    "# Undersample Empty Images\n",
    "Here we undersample the empty images to get a better balanced group with more ships to try and segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "0cf0bb261eda957cb0a12a330260e1390c57c8c9"
   },
   "outputs": [],
   "source": [
    "train_df['grouped_ship_count'] = train_df['ships'].map(lambda x: (x+1)//2).clip(0, 7)\n",
    "    \n",
    "balanced_train_df = train_df.groupby('grouped_ship_count').apply(sample_ships)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a3fb9fe33d81374c7bd836f5bc86a1df89190805"
   },
   "source": [
    "# Decode all the RLEs into Images\n",
    "We make a generator to produce batches of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "1983738da75b031f2bec8ba36db01c095e7c5d59"
   },
   "outputs": [],
   "source": [
    "train_gen = make_image_gen(balanced_train_df)\n",
    "train_x, train_y = next(train_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8f47639c987a10ebcb53e51f55aa8a11c98fa860"
   },
   "source": [
    "# Make the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "30cb02a2a7103a9d66e90f701991199de1e5b73e"
   },
   "outputs": [],
   "source": [
    "valid_x, valid_y = next(make_image_gen(valid_df, VALID_IMG_COUNT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a8f65e7942816fb75b687a549dc1d5cc48d00e21"
   },
   "source": [
    "# Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "dg_args = dict(featurewise_center = False, \n",
    "                  samplewise_center = False,\n",
    "                  rotation_range = 15, \n",
    "                  width_shift_range = 0.1, \n",
    "                  height_shift_range = 0.1, \n",
    "                  shear_range = 0.01,\n",
    "                  zoom_range = [0.9, 1.25],  \n",
    "                  horizontal_flip = True, \n",
    "                  vertical_flip = True,\n",
    "                  fill_mode = 'reflect',\n",
    "                   data_format = 'channels_last')\n",
    "# brightness can be problematic since it seems to change the labels differently from the images \n",
    "if AUGMENT_BRIGHTNESS:\n",
    "    dg_args[' brightness_range'] = [0.5, 1.5]\n",
    "image_gen = ImageDataGenerator(**dg_args)\n",
    "\n",
    "if AUGMENT_BRIGHTNESS:\n",
    "    dg_args.pop('brightness_range')\n",
    "label_gen = ImageDataGenerator(**dg_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "6122ccb9e58bfac6fa5e11c86121e78d9e5151b1"
   },
   "outputs": [],
   "source": [
    "cur_gen = create_aug_gen(train_gen)\n",
    "t_x, t_y = next(cur_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "33300c4f03b6600da7b418f775d11d7ebf76a35a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba08494eb9736ec3556b7c879143cdcdea89febf"
   },
   "source": [
    "# Build a Model\n",
    "Here we use a slight deviation on the U-Net standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "2687377309d3cbbab1197f4eccd2b50ab996f5a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Build U-Net model\n",
    "if UPSAMPLE_MODE=='DECONV':\n",
    "    upsample=upsample_conv\n",
    "else:\n",
    "    upsample=upsample_simple\n",
    "    \n",
    "input_img = layers.Input(t_x.shape[1:], name = 'RGB_Input')\n",
    "pp_in_layer = input_img\n",
    "if NET_SCALING is not None:\n",
    "    pp_in_layer = layers.AvgPool2D(NET_SCALING)(pp_in_layer)\n",
    "    \n",
    "pp_in_layer = layers.GaussianNoise(GAUSSIAN_NOISE)(pp_in_layer)\n",
    "pp_in_layer = layers.BatchNormalization()(pp_in_layer)\n",
    "\n",
    "c1 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (pp_in_layer)\n",
    "c1 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\n",
    "p1 = layers.MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "c2 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\n",
    "c2 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\n",
    "p2 = layers.MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "c3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\n",
    "c3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\n",
    "p3 = layers.MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\n",
    "c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\n",
    "p4 = layers.MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "\n",
    "c5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same') (p4)\n",
    "c5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same') (c5)\n",
    "\n",
    "u6 = upsample(64, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "u6 = layers.concatenate([u6, c4])\n",
    "c6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (u6)\n",
    "c6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (c6)\n",
    "\n",
    "u7 = upsample(32, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "u7 = layers.concatenate([u7, c3])\n",
    "c7 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (u7)\n",
    "c7 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (c7)\n",
    "\n",
    "u8 = upsample(16, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "u8 = layers.concatenate([u8, c2])\n",
    "c8 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (u8)\n",
    "c8 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (c8)\n",
    "\n",
    "u9 = upsample(8, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "u9 = layers.concatenate([u9, c1], axis=3)\n",
    "c9 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (u9)\n",
    "c9 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (c9)\n",
    "\n",
    "d = layers.Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "d = layers.Cropping2D((EDGE_CROP, EDGE_CROP))(d)\n",
    "d = layers.ZeroPadding2D((EDGE_CROP, EDGE_CROP))(d)\n",
    "if NET_SCALING is not None:\n",
    "    d = layers.UpSampling2D(NET_SCALING)(d)\n",
    "\n",
    "seg_model = models.Model(inputs=[input_img], outputs=[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "1678069aa8013510264ba898291c6ae2dce88a76"
   },
   "outputs": [],
   "source": [
    "seg_model.compile(optimizer='rmsprop', \n",
    "                  loss=dice_p_bce, \n",
    "                  metrics=[dice_coef, 'binary_accuracy', true_positive_rate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "5b67d808c0b8c7e28bff41e6d3858ff6f09dd626"
   },
   "outputs": [],
   "source": [
    "step_count = min(MAX_TRAIN_STEPS, balanced_train_df.shape[0]//BATCH_SIZE)\n",
    "aug_gen = create_aug_gen(make_image_gen(balanced_train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "5b67d808c0b8c7e28bff41e6d3858ff6f09dd626",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "200/200 [==============================] - 297s 1s/step - loss: -0.1593 - dice_coef: 0.1594 - binary_accuracy: 0.9777 - true_positive_rate: nan - val_loss: -0.7541 - val_dice_coef: 0.7542 - val_binary_accuracy: 0.9989 - val_true_positive_rate: nan\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 230s 1s/step - loss: -0.1093 - dice_coef: 0.1094 - binary_accuracy: 0.9966 - true_positive_rate: nan - val_loss: -0.7546 - val_dice_coef: 0.7546 - val_binary_accuracy: 0.9989 - val_true_positive_rate: nan\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 266s 1s/step - loss: -0.1342 - dice_coef: 0.1343 - binary_accuracy: 0.9956 - true_positive_rate: nan - val_loss: -0.7557 - val_dice_coef: 0.7557 - val_binary_accuracy: 0.9989 - val_true_positive_rate: nan\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 263s 1s/step - loss: -0.1655 - dice_coef: 0.1655 - binary_accuracy: 0.9963 - true_positive_rate: nan - val_loss: -0.7553 - val_dice_coef: 0.7554 - val_binary_accuracy: 0.9989 - val_true_positive_rate: nan\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 271s 1s/step - loss: -0.1324 - dice_coef: 0.1324 - binary_accuracy: 0.9965 - true_positive_rate: nan - val_loss: -0.7559 - val_dice_coef: 0.7559 - val_binary_accuracy: 0.9989 - val_true_positive_rate: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f40158e4f28>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_model.fit_generator(aug_gen, \n",
    "                        steps_per_epoch=step_count, \n",
    "                        epochs=NB_EPOCHS, \n",
    "                        validation_data=(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "ce1167e9f09200f537e61f93f486168a13be1711"
   },
   "outputs": [],
   "source": [
    "seg_model.save('seg_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### if you use custom functions (e.g. loss function) you need to adjust custom_objects parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model = models.load_model('seg_model.h5', custom_objects={'dice_p_bce': dice_p_bce,\n",
    "                                                             'dice_coef': dice_coef,\n",
    "                                                             'true_positive_rate': true_positive_rate})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0018ab172d18936f8cc2c5df33d2f840dc16bf4f"
   },
   "source": [
    "# Prepare Full Resolution Model\n",
    "Here we account for the scaling so everything can happen in the model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "17408f0ee8dc16149b8eff0447a1427ab3ed82ba",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if IMG_SCALING is not None:\n",
    "    fullres_model = models.Sequential()\n",
    "    fullres_model.add(layers.AvgPool2D(IMG_SCALING, input_shape = (None, None, 3)))\n",
    "    fullres_model.add(seg_model)\n",
    "    fullres_model.add(layers.UpSampling2D(IMG_SCALING))\n",
    "else:\n",
    "    fullres_model = seg_model\n",
    "fullres_model.save('fullres_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "17edb177402ae51651692511827a7e9d60646533"
   },
   "source": [
    "# Run the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "4911811f267f9f3397a58902da9e75c6f261ad40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88500 test images found\n"
     ]
    }
   ],
   "source": [
    "test_paths = os.listdir(test_image_dir)\n",
    "print(len(test_paths), 'test images found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55b31613bce4cc58b86edf2b8935e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_pred_rows = []\n",
    "\n",
    "for c_img_name in tqdm_notebook(test_paths):\n",
    "    c_path = os.path.join(test_image_dir, c_img_name)\n",
    "    c_img = imread(c_path)\n",
    "    c_img = np.expand_dims(c_img, 0)/255.0\n",
    "    cur_seg = seg_model.predict(c_img)[0]\n",
    "    cur_seg = binary_opening(cur_seg>0.5, np.expand_dims(disk(2), -1))\n",
    "    cur_rles = multi_rle_encode(cur_seg)\n",
    "    if len(cur_rles)>0:\n",
    "        for c_rle in cur_rles:\n",
    "            out_pred_rows += [{'ImageId': c_img_name, 'EncodedPixels': c_rle}]\n",
    "    else:\n",
    "        out_pred_rows += [{'ImageId': c_img_name, 'EncodedPixels': None}]\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(out_pred_rows)[['ImageId', 'EncodedPixels']]\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "submission_df.sample(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
